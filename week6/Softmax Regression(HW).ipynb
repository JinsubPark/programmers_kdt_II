{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "mnist.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = mnist[\"data\"], mnist[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.fit(y[:,np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = enc.transform(y[:,np.newaxis]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold out validation\n",
    "X_train, X_test, y_train, y_test = X[:60000], X[60000:], Y[:60000], Y[60000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X, W):\n",
    "    K = np.size(W, axis=1)\n",
    "    A = np.exp(X @ W)\n",
    "    B = np.diag(1 / (np.reshape(A @ np.ones((K,1)), -1)))\n",
    "    Y = B @ A\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.01, 0.99],\n",
       "       [0.  , 0.  , 0.  , 1.  ],\n",
       "       [0.  , 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understaning softmax function\n",
    "X = np.array([\n",
    "    [1, 1, 1], \n",
    "    [2, 2, 2], \n",
    "    [3, 3, 3]\n",
    "])\n",
    "W = np.array([\n",
    "    [0.5, 1, 2, 5],\n",
    "    [1, 2, 3, 4], \n",
    "    [0.3, 1, 2, 3]\n",
    "])\n",
    "\n",
    "np.around(softmax(X, W), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, T, W, lamb):\n",
    "    epsilon = 1e-5\n",
    "    N = len(T)\n",
    "    K = np.size(T, 1)\n",
    "    # cost1: the sum of the all elements\n",
    "    cost1 = - (1/N) * np.ones((1,N)) @ (np.multiply(np.log(softmax(X, W) + epsilon), T)) @ np.ones((K,1))\n",
    "    cost2 = 1/(2*N) * (lamb * np.linalg.norm(W, ord='fro'))  ## add\n",
    "    cost = cost1+cost2\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    return np.argmax((X @ W), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(X, T, W, lamb, learning_rate, iterations, batch_size):\n",
    "    N = len(T)\n",
    "    cost_history = np.zeros((iterations,1))\n",
    "    shuffled_indices = np.random.permutation(N)\n",
    "    X_shuffled = X[shuffled_indices]\n",
    "    T_shuffled = T[shuffled_indices]\n",
    "\n",
    "    for i in range(iterations):\n",
    "        j = i % N\n",
    "        X_batch = X_shuffled[j:j+batch_size]\n",
    "        T_batch = T_shuffled[j:j+batch_size]\n",
    "        # batch가 epoch 경계를 넘어가는 경우, 앞 부분으로 채워줌\n",
    "        if X_batch.shape[0] < batch_size:\n",
    "            X_batch = np.vstack((X_batch, X_shuffled[:(batch_size - X_batch.shape[0])]))\n",
    "            T_batch = np.vstack((T_batch, T_shuffled[:(batch_size - T_batch.shape[0])]))\n",
    "        W = W - (learning_rate/batch_size) * (X_batch.T @ (softmax(X_batch, W) - T_batch) + lamb * W)\n",
    "#         cost_history[i] = compute_cost(X_batch, T_batch, W, lamb)\n",
    "#         if i % 1000 == 0:\n",
    "#             print(cost_history[i][0])\n",
    "\n",
    "    return W        #  (cost_history, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_predict_by_lambda(lamb):\n",
    "    X = np.hstack((np.ones((np.size(X_train, 0),1)),X_train))\n",
    "    T = y_train\n",
    "\n",
    "    K = np.size(T, 1)\n",
    "    M = np.size(X, 1)\n",
    "    W = np.zeros((M,K))\n",
    "\n",
    "    iterations = 50000\n",
    "    learning_rate = 0.01\n",
    "\n",
    "    # initial_cost = compute_cost(X, T, W, lamb)\n",
    "\n",
    "    # print(\"Initial Cost is: {} \\n\".format(initial_cost[0][0]))\n",
    "\n",
    "    W_optimal = batch_gd(X, T, W, lamb, learning_rate, iterations, 64)\n",
    "    \n",
    "    ## Accuracy\n",
    "    X_ = np.hstack((np.ones((np.size(X_test, 0),1)),X_test))\n",
    "    T_ = y_test\n",
    "    y_pred = predict(X_, W_optimal)\n",
    "    val_acc = float(sum(y_pred == np.argmax(T_, axis=1)))/ float(len(y_test))\n",
    "\n",
    "    print('-' * 30)\n",
    "    print('lambda:', lamb)\n",
    "    print('val_acc:', val_acc)\n",
    "    print('-' * 30)\n",
    "    return val_acc, lamb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "lambda: 0.7684596415095928\n",
      "val_acc: 0.89\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7202291450471682\n",
      "val_acc: 0.8858\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.6301098410615177\n",
      "val_acc: 0.8917\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.8054972548023436\n",
      "val_acc: 0.8819\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.3315271046160003\n",
      "val_acc: 0.9058\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.5054354245752756\n",
      "val_acc: 0.8961\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.45167305857787643\n",
      "val_acc: 0.8975\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.785832210111468\n",
      "val_acc: 0.8992\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.651244758360795\n",
      "val_acc: 0.9008\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.9730105394584231\n",
      "val_acc: 0.8897\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.9089627639944585\n",
      "val_acc: 0.8884\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.8250811566293287\n",
      "val_acc: 0.8871\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.8872527442006283\n",
      "val_acc: 0.8866\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.8043732270289082\n",
      "val_acc: 0.8957\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.37240633100352305\n",
      "val_acc: 0.8942\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.32475502731025685\n",
      "val_acc: 0.8953\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.0007938857090212803\n",
      "val_acc: 0.9174\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.04275433189927058\n",
      "val_acc: 0.9161\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.5161008788642684\n",
      "val_acc: 0.9013\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7818185570384941\n",
      "val_acc: 0.8925\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7095691107750959\n",
      "val_acc: 0.8902\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.9600749922710115\n",
      "val_acc: 0.8939\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7993226719748793\n",
      "val_acc: 0.8899\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.05515243886131049\n",
      "val_acc: 0.9131\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.686299383890391\n",
      "val_acc: 0.8921\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.3129876886340325\n",
      "val_acc: 0.9038\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.45498136915003384\n",
      "val_acc: 0.8999\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.2770334556745314\n",
      "val_acc: 0.8913\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.3220239216402687\n",
      "val_acc: 0.9008\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.5445039779556302\n",
      "val_acc: 0.8953\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.2628139466139924\n",
      "val_acc: 0.903\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.6167943070531773\n",
      "val_acc: 0.8807\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7900690744462163\n",
      "val_acc: 0.8801\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.8300165808296905\n",
      "val_acc: 0.8872\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.6893661393433651\n",
      "val_acc: 0.8922\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.6314052381008499\n",
      "val_acc: 0.8962\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.23462011457984844\n",
      "val_acc: 0.9031\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.5693053997683484\n",
      "val_acc: 0.8983\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.6290663508990408\n",
      "val_acc: 0.8955\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.9223104972725031\n",
      "val_acc: 0.8868\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7448425260813157\n",
      "val_acc: 0.892\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.8877667547318839\n",
      "val_acc: 0.8902\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.2411831898294945\n",
      "val_acc: 0.9054\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.450625803813515\n",
      "val_acc: 0.9014\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.8932227513711646\n",
      "val_acc: 0.8941\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.11464285401410002\n",
      "val_acc: 0.911\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7448935759131289\n",
      "val_acc: 0.8937\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.7384976848100192\n",
      "val_acc: 0.8955\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.631659758969434\n",
      "val_acc: 0.9004\n",
      "------------------------------\n",
      "------------------------------\n",
      "lambda: 0.33300638020411555\n",
      "val_acc: 0.8996\n",
      "------------------------------\n",
      "----------------------------------------\n",
      "best lambda: 0.0007938857090212803\n",
      "best accurcy: 0.9174\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "lambs = np.random.uniform(low=0, high=1, size=50)\n",
    "best_acc = 0\n",
    "best_lamb = 0\n",
    "\n",
    "for lamb in lambs:\n",
    "    tem_acc, tem_lamb = fit_predict_by_lambda(lamb)\n",
    "    if tem_acc > best_acc:\n",
    "        best_lamb = tem_lamb\n",
    "        best_acc = tem_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "best lambda: 0.0007938857090212803\n",
      "best accurcy: 0.9174\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-' * 40)\n",
    "print('best lambda:', best_lamb)\n",
    "print('best accurcy:', best_acc)\n",
    "print('-' * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
